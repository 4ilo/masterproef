@article{Fang2003,
abstract = {In a visual driver-assistance system, road-sign detection and tracking is one of the major tasks. This study describes an approach to detecting and tracking road signs appearing in complex traffic scenes. In the detection phase, two neural networks are developed to extract color and shape features of traffic signs from the input scenes images. Traffic signs are then located in the images based on the extracted features. This process is primarily conceptualized in terms of fuzzy-set discipline. In the tracking phase, traffic signs located in the previous phase are tracked through image sequences using a Kalman filter. The experimental results demonstrate that the proposed method performs well in both detecting and tracking road signs present in complex scenes and in various weather and illumination conditions.},
annote = {Zoekt naar edges en spesifieke hue apart. Voegt daarna deze 2 samen en vergelijkt dit met mogelijke boden},
author = {Fang, Chiung Yao and Chen, Sei Wang and Fuh, Chiou Shann},
doi = {10.1109/TVT.2003.810999},
isbn = {0018-9545},
issn = {00189545},
journal = {IEEE Transactions on Vehicular Technology},
keywords = {(HSI) color model,Fuzzy integration,Hue,Intensity,Kalman filter,Neural networks,Road-sign detection and tracking,Saturation},
number = {5},
pages = {1329--1341},
title = {{Road-sign detection and tracking}},
volume = {52},
year = {2003}
}
@article{Zabihi2017,
author = {Zabihi, S. J. and Zabihi, S. M. and Beauchemin, S. S. and Bauer, M. A.},
doi = {10.1109/IVS.2017.7995781},
isbn = {9781509048045},
issn = {0022-4596},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {583--588},
title = {{Detection and recognition of traffic signs inside the attentional visual field of drivers}},
year = {2017}
}
@inproceedings{Lowe1999,
author = {Lowe, D G},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790410},
month = {sep},
pages = {1150--1157 vol.2},
title = {{Object recognition from local scale-invariant features}},
volume = {2},
year = {1999}
}
@article{zhangwall,
  title={Wall, Floor, Ceiling, Object Region Identification from Single Image},
  author={Zhang, Zhong-Ju},
  publisher={Citeseer}
}
@article{Li2010,
abstract = {We present a novel method for image-based floor detection from a single image. In contrast with previous approaches that rely upon homographies, our approach does not require multiple images (either stereo or optical flow). It also does not require the camera to be calibrated, even for lens distortion. The technique combines three visual cues for evaluating the likelihood of horizontal intensity edge line segments belonging to the wall-floor boundary. The combination of these cues yields a robust system that works even in the presence of severe specular reflections, which are common in indoor environments. The nearly real-time algorithm is tested on a large database of images collected in a wide variety of conditions, on which it achieves nearly 90{\%} detection accuracy.},
author = {Li, Yinxiao and Birchfield, Stanley T.},
doi = {10.1109/IROS.2010.5652818},
isbn = {9781424466757},
issn = {2153-0858},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
pages = {837--843},
title = {{Image-based segmentation of indoor corridor floors for a mobile robot}},
year = {2010}
}
@article{Redmon_2016,
   title={You Only Look Once: Unified, Real-Time Object Detection},
   ISBN={9781467388511},
   url={http://dx.doi.org/10.1109/CVPR.2016.91},
   DOI={10.1109/cvpr.2016.91},
   journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
   year={2016},
   month={Jun}
}
@ARTICLE{Canny, 
author={J. Canny}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={A Computational Approach to Edge Detection}, 
year={1986}, 
volume={PAMI-8}, 
number={6}, 
pages={679-698}, 
keywords={Image edge detection;Detectors;Machine vision;Shape measurement;Performance analysis;Uncertainty;Gaussian approximation;Signal to noise ratio;Signal synthesis;Feature extraction;Edge detection;feature extraction;image processing;machine vision;multiscale image analysis}, 
doi={10.1109/TPAMI.1986.4767851}, 
ISSN={0162-8828}, 
month={Nov},}

@article{Rodriguez-Telles2013,
abstract = {We present a novel technique that robustly segments free-space for robot navigation purposes. In particular, we are interested in a reactive visual navigation, in which the rapid and accurate detection of free space where the robot can navigate is crucial. Contrary to existing methods that use multiple cameras in different configurations, we use a downward-facing monocular camera to search for free space in a large and complicated room environment. The proposed approach combines two techniques. First, we apply the Simple Linear Iterative Clustering super-pixel algorithm to the input images. Then, by relying on particular characteristics of floor super pixels, we use a simple classification method based on a normalized SSD similarity measure to group together those super pixels that belongs to the floor (considered as free space). The method intermittently examines low resolution images (80 × 60) in the CIE Lab color model. Experimental results show that our segmentation approach is robust, even in the presence of severe specular reflections and allows for real-time navigation. {\textcopyright} 2013 IEEE.},
author = {Rodr{\'{i}}guez-Telles, F. Geovani and Torres-M{\'{e}}ndez, L. Abril and Mart{\'{i}}nez-Garc{\'{i}}a, Edgar A.},
doi = {10.1109/CRV.2013.40},
isbn = {9780769549835},
journal = {Proceedings - 2013 International Conference on Computer and Robot Vision, CRV 2013},
keywords = {SLIC superpixels,segmentation},
pages = {167--173},
title = {{A fast floor segmentation algorithm for visual-based robot navigation}},
year = {2013}
}
@ARTICLE{slic, 
author={R. Achanta and A. Shaji and K. Smith and A. Lucchi and P. Fua and S. Süsstrunk}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={SLIC Superpixels Compared to State-of-the-Art Superpixel Methods}, 
year={2012}, 
volume={34}, 
number={11}, 
pages={2274-2282}, 
keywords={computer vision;image segmentation;iterative methods;pattern clustering;SLIC superpixels;computer vision;image boundary;memory efficiency;segmentation performance;simple linear iterative clustering;k-means clustering approach;superpixel generation;supervoxel generation;Clustering algorithms;Image segmentation;Complexity theory;Image color analysis;Image edge detection;Measurement uncertainty;Approximation algorithms;Superpixels;segmentation;clustering;k-means;Algorithms;Image Enhancement;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted}, 
doi={10.1109/TPAMI.2012.120}, 
ISSN={0162-8828}, 
month={Nov},}

@article{Badrinarayanan,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.00561v3},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto and Member, Senior},
eprint = {arXiv:1511.00561v3},
pages = {1--14},
title = {{SegNet : A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}}
}

@InProceedings{Song_2015_CVPR,
author = {Song, Shuran and Lichtenberg, Samuel P. and Xiao, Jianxiong},
title = {SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@article{Llopart2017,
abstract = {In this paper we present a new method that robustly identifies doors, cabinets and their respective handles, with special emphasis on extracting useful features from handles to be then manipulated. The novelty of this system relies on the combination of a Convolutional Neural Net (CNN), as a form of reducing the search space, several methods to extract point cloud data and a mobile robot to interact with the objects. The framework consists of the following components: The implementation of a CNN to extract a Region of Interest (ROI) from an image corresponding to a door or cabinet. Several vision based techniques to detect handles inside the ROI and its 3D positioning. A complementary plane segmentation method to differentiate door/cabinet from the handle. An algorithm to fuse both approaches robustly and extract essential information from the handle for robotic grasping (i.e. handle point cloud, door plane model, grasping locations, turning orientation, orthogonal vector to door). A mobile robot for grasping the handle. The system assumes no prior knowledge of the environment.},
author = {Llopart, Adrian and Ravn, Ole and Andersen, Nils A.},
doi = {10.1109/ICCAR.2017.7942676},
isbn = {9781509060870},
journal = {2017 3rd International Conference on Control, Automation and Robotics, ICCAR 2017},
keywords = {Convolutional neural network,Door recognition,Image processing,Mobile robot,Point cloud processing},
pages = {144--149},
title = {{Door and cabinet recognition using Convolutional Neural Nets and real-time method fusion for handle detection and grasping}},
year = {2017}
}

@inproceedings{Tomono2000,
author = {Tomono, M and Yuta, S},
booktitle = {Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)},
doi = {10.1109/ROBOT.2000.844076},
issn = {1050-4729},
keywords = {mobile robots;computerised navigation;robot vision},
month = {apr},
pages = {313--320 vol.1},
title = {{Mobile robot navigation in indoor environments using object and character recognition}},
volume = {1},
year = {2000}
}
@INPROCEEDINGS{Henry10rgb-dmapping,
    author = {Peter Henry and Michael Krainin and Evan Herbst and Xiaofeng Ren and Dieter Fox},
    title = {RGB-D Mapping: Using depth cameras for dense 3D modeling of indoor environments},
    booktitle = {In the 12th International Symposium on Experimental Robotics (ISER},
    year = {2010}
}
@INPROCEEDINGS{schmid2013, 
author={K. Schmid and T. Tomic and F. Ruess and H. Hirschmüller and M. Suppa}, 
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
title={Stereo vision based indoor/outdoor navigation for flying robots}, 
year={2013}, 
volume={}, 
number={}, 
pages={3955-3962}, 
keywords={aerospace computing;aerospace control;aerospace robotics;cameras;collision avoidance;control engineering computing;field programmable gate arrays;image fusion;image matching;robot vision;state estimation;stereo image processing;autonomously flying system;path planning;on-board 3D mapping;system state estimation;vision pipeline;time delays;IMU data fusion;keyframe-based stereo odometry;semiglobal matching algorithm;stereo image processing;FPGA board;processor;IMU;inertial measurement unit;stereo camera pair;mechanically damped perception unit;flight control;obstacle avoidance;unknown indoor-outdoor environments;autonomous waypoint navigation;quadrotor platform;flying robots;stereo vision based indoor-outdoor navigation;Navigation;Cameras;Robot sensing systems;Visualization;Field programmable gate arrays;Three-dimensional displays}, 
doi={10.1109/IROS.2013.6696922}, 
ISSN={2153-0858}, 
month={Nov},}

@article{Zhou2009,
abstract = {A scale invariant feature transform (SIFT) based mean shift algorithm is presented for object tracking in real scenarios. SIFT features are used to correspond the region of interests across frames. Meanwhile, mean shift is applied to conduct similarity search via color histograms. The probability distributions from these two measurements are evaluated in an expectation-maximization scheme so as to achieve maximum likelihood estimation of similar regions. This mutual support mechanism can lead to consistent tracking performance if one of the two measurements becomes unstable. Experimental work demonstrates that the proposed mean shift/SIFT strategy improves the tracking performance of the classical mean shift and SIFT tracking algorithms in complicated real scenarios. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zhou, Huiyu and Yuan, Yuan and Shi, Chunmei},
doi = {10.1016/j.cviu.2008.08.006},
eprint = {arXiv:1011.1669v3},
isbn = {1077-3142},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Color histogram,Expectation-maximization,Mean shift,Object tracking,SIFT features},
number = {3},
pages = {345--352},
pmid = {12824425},
publisher = {Elsevier Inc.},
title = {{Object tracking using SIFT features and mean shift}},
url = {http://dx.doi.org/10.1016/j.cviu.2008.08.006},
volume = {113},
year = {2009}
}
@article{Baheti2016,
abstract = {AIMS: To determine the cause of an outbreak of acute gastroenteritis that occurred on a cruise ship sailing along the Yangzi River from Chongqing to Nanjing, China. METHODS AND RESULTS: Noroviruses were identified by reverse transcription-PCR (RT-PCR) in rectal swabs from 34 of 54 subjects tested (63.0{\%}). Sequencing and genotyping showed that noroviruses of up to seven different genotypes circulated in this outbreak: noroviruses GI.1, GI.2, GI.3, GI.4, GI.8, GI.9 and an uncommon strain GII.17. Common genotypes were not identified in this event. None of the food or water samples were tested positive for noroviruses. CONCLUSIONS: We suspected that it was a point-source infection due to contaminated water or food harvested from contaminated water, taking account of the co-existence of diverse norovirus genotypes. SIGNIFICANCE AND IMPACT OF THE STUDY: In this study, we presented the molecular investigation of a norovirus outbreak on a cruise in China. We revealed that the outbreak was caused by several different norovirus genotypes and analysed the possible source of infection as well, thus facilitating the evaluation of epidemiological issues regarding noroviruses in this area.},
author = {Baheti, Bhakti and Baid, Ujjwal and Talbar, Sanjay},
doi = {10.1109/CASP.2016.7746175},
isbn = {9781509008490},
journal = {Conference on Advances in Signal Processing, CASP 2016},
keywords = {KLT,Mean Shift,RANSAC,SIFT},
pages = {254--259},
pmid = {26481457},
title = {{An approach to automatic object tracking system by combination of SIFT and RANSAC with mean shift and KLT}},
year = {2016}
}
@article{tomasi1991detection,
  title={Detection and tracking of point features},
  author={Tomasi, Carlo and Kanade, Takeo},
  year={1991},
  publisher={School of Computer Science, Carnegie Mellon Univ. Pittsburgh}
}

@article{Ning2017,
annote = {Object tracking/prediction with yolo},
archivePrefix = {arXiv},
arxivId = {arXiv:1607.05781v1},
author = {Ning, Guanghan},
eprint = {arXiv:1607.05781v1},
isbn = {9781467368537},
number = {1},
pages = {1--4},
title = {{Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking}},
year = {2017}
}
